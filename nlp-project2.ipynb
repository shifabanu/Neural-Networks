{"cells":[{"metadata":{},"cell_type":"markdown","source":"**EE258 Neural Networks**\nProject-2\nFall 2020\n\n\nBy:\n1)Rojin Zandi\n2)Shifa Shaikh"},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport re\nimport string\nfrom collections import Counter\nimport nltk\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import confusion_matrix as CM\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Reading data from the directories"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\") #train\n\ntest=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\") #test\n\nsub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv') #submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking GPU availability\n\nimport torch\nif torch.cuda.is_available():  \n    device = torch.device(\"cuda\")\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Description and Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head() #Checking the first 5 rows in the train data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe() #analysis of numerical values in the train data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape #train data dimension","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape #test data shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head() #Checking the first 5 rows in the test data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count-plot of train set \n\nplt.figure(figsize=(9,6))\nsns.countplot(y=data.keyword, order = data.keyword.value_counts().iloc[:10].index)\nplt.title('Top 10 keywords')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bar plot of disaster tweets and non-disaster tweets\n\nkw_d = data[data.target==1].keyword.value_counts().head(10) #First 10 Disaster tweets \nkw_nd = data[data.target==0].keyword.value_counts().head(10) #First 10 Non-disaster tweets\n\nplt.figure(figsize=(13,5))\n\n#Plot-1 for disaster tweets\nplt.subplot(121)\nsns.barplot(kw_d, kw_d.index, color='c')\nplt.title('Top keywords for disaster tweets')\n\n#Plot-2 for Non-disaster tweets\nplt.subplot(122)\nsns.barplot(kw_nd, kw_nd.index, color='y')\nplt.title('Top keywords for non-disaster tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntop_d = data.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\ntop_nd = data.groupby('keyword').mean()['target'].sort_values().head(10)\n\nplt.figure(figsize=(13,5))\n\nplt.subplot(121)\nsns.barplot(top_d, top_d.index, color='pink')\nplt.title('Keywords with highest % of disaster tweets')\n\nplt.subplot(122)\nsns.barplot(top_nd, top_nd.index, color='yellow')\nplt.title('Keywords with lowest % of disaster tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common locations in the tweets\n\nplt.figure(figsize=(9,6))\nsns.countplot(y=data.location, order = data.location.value_counts().iloc[:15].index)\nplt.title('Top 15 locations')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#locations plot\n\nraw_loc = data.location.value_counts()\ntop_loc = list(raw_loc[raw_loc>=10].index)\ntop_only = data[data.location.isin(top_loc)]\n\ntop_l = top_only.groupby('location').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l.index, y=top_l)\nplt.axhline(np.mean(data.target))\nplt.xticks(rotation=80)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Data Cleaning & Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#data-cleaning\n\ndef clean_text(text):\n    \n    sw = stopwords.words('english') #Stopwords from NLTK.Corpus\n    \n    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n    \n    html=re.compile(r'<.*?>') \n    \n    text = html.sub(r'',text) #Removing html tags\n    \n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n    for p in punctuations:\n        text = text.replace(p,'') #Removing punctuations\n        \n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    \n    text = \" \".join(text) #removing stopwords\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text) #Removing emojis\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of mispelled words\n\nmispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reference - https://www.kaggle.com/mlwhiz/textcnn-pytorch-and-keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting the misspell words\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\n\n#replacing the missepells\ndef replace_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying the changes to the train data\n\ndata['text'] = data['text'].str.lower() #convert everything to lower-case words\n\ndata['text'] = data['text'].apply(lambda x: replace_misspell(x)) #replace the misspell words\n\ndata['text'] = data['text'].apply(lambda x: clean_text(x)) #clean the data using the above function\n\n\n\ndata.head(3) #check the changes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#repeat the same process for the test data too\n\ntest['text'] = test['text'].str.lower()\n\ntest['text'] = test['text'].apply(lambda x: replace_misspell(x))\n\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n\n\ntest.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data: 80% training + 20% validation data\n\ntraining_size = 6090\n\nx_train = data.text[0:training_size] #train input data\ny_train = data.target[0:training_size] #train output\n\nvalid_sentences = data.text[training_size:] #validation input data\nvalid_labels = data.target[training_size:] #validation output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tokenization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#pre-processing\n#generate vectors\n\ntokenizer = Tokenizer(num_words=None)\ntokenizer.fit_on_texts(x_train)\n\nword_index = tokenizer.word_index\nnum_words = len(tokenizer.word_index) + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert into sequence and pad it\n\ntraining_sequences = tokenizer.texts_to_sequences(x_train)\ntraining_padded = pad_sequences(training_sequences, maxlen=20, padding='post', truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similarly, vectorizing and padding for the validation data.\n\nvalidation_sequences = tokenizer.texts_to_sequences(valid_sentences)\nvalidation_padded = pad_sequences(validation_sequences, maxlen=20, padding='post', truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similarly, vectorizing and padding for the test data.\n\ntest_sequences = tokenizer.texts_to_sequences(test.text)\ntest_padded = pad_sequences(test_sequences, maxlen=20, padding='post', truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# decode function to convert integer values back to text sequences to check the result\n\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndef decode(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Model-1"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = tf.keras.Sequential([\ntf.keras.layers.Embedding(num_words, 100),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,recurrent_dropout=0.1)),\ntf.keras.layers.Dense(64, activation='relu'),\ntf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel1.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the Model-1\n\nEPOCHS = 15\nhistory1 = model1.fit(training_padded, y_train, validation_data=(validation_padded, valid_labels), batch_size=30, epochs=EPOCHS, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Accuracy and Loss of Training and Validation set\n\ndef plot_history(history, metric, val_metric,EPOCHS):\n    acc = history.history[metric]\n    val_acc = history.history[val_metric]\n    loss=history.history['loss']\n    val_loss=history.history['val_loss']\n\n    epochs_range = range(EPOCHS)\n\n    fig=plt.figure(figsize=(12, 10))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.ylim([min(plt.ylim()),1])\n    plt.grid(True)\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.grid(True)\n    plt.title('Training and Validation Loss')\n    plt.show()\n\n\nplot_history(history1,'accuracy','val_accuracy',EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict using the trained Model-1\n\npred1 = model1.predict_classes(validation_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Measuring the performance using various metrics: Precision, Recall, F1 score, Overall Accuracy\n\ncnf_matrix = confusion_matrix(valid_labels,pred1)\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');\n\nprecision_scoreM1=precision_score(valid_labels,pred1, average=None)\nprint(\"\\n Model-1 Precision_score\\n\", precision_scoreM1)\n\nrecall_scoreM1=recall_score(valid_labels,pred1, average=None)\nprint(\"\\n Model-1 Recall_score\\n\",recall_scoreM1)\n\naccuracy_scoreM1=accuracy_score(valid_labels,pred1)\nprint(\"\\n Model-1 Accuracy_score\\n\",accuracy_scoreM1)\n\nf1_scoreM1=f1_score(valid_labels,pred1, average=None)\nprint(\"\\n Model-1 F1_score\\n\",f1_scoreM1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Model-2\n\nUsing a pre-trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove6b100dtxt/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_na = []\nembedding_matrix = np.zeros((num_words,100))\nword_index = tokenizer.word_index\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    emb_vec=embedding_dict.get(word)\n    \n    if emb_vec is None:\n        \n        \n        words_na.append(word)\n    \n    elif emb_vec is not None:\n        embedding_matrix[i]=emb_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Out of vocabulory words:\",len(words_na))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = tf.keras.Sequential([\ntf.keras.layers.Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),trainable=False),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\ntf.keras.layers.Dense(64, activation='relu'),\ntf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel2.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train Model-2\n\nhistory21 = model2.fit(training_padded, y_train,epochs=EPOCHS,batch_size=12, validation_data=(validation_padded, valid_labels), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot the results of Model-2\n\nplot_history(history21,'accuracy','val_accuracy',EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict using the trained Model-2\n\npred21 = model2.predict_classes(validation_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Measuring the performance using various metrics: Precision, Recall, F1 score, Overall Accuracy\n\ncnf_matrix = confusion_matrix(valid_labels,pred21)\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');\n\n\nprecision_scoreM2=precision_score(valid_labels,pred21, average=None)\nprint(\"\\n Model-21 Precision_score\\n\", precision_scoreM2)\n\nrecall_scoreM2=recall_score(valid_labels,pred21, average=None)\nprint(\"\\n Model-21 Recall_score\\n\",recall_scoreM2)\n\naccuracy_scoreM2=accuracy_score(valid_labels,pred21)\nprint(\"\\n Model-21 Accuracy_score\\n\",accuracy_scoreM2)\n\nf1_scoreM2=f1_score(valid_labels,pred21, average=None)\nprint(\"\\n Model-21 F1_score\\n\",f1_scoreM2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using a Regularization technique called \"Early Stopping\""},{"metadata":{"trusted":true},"cell_type":"code","source":"#configure the early-stop\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0,restore_best_weights=True, mode='auto')\n\n#train the Model-2 again but this time with early-stop\nhistory22 = model2.fit(training_padded, y_train,epochs=EPOCHS,batch_size=12, validation_data=(validation_padded, valid_labels), verbose=2,callbacks=[earlystop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot results of early-stop\nplot_history(history22,'accuracy','val_accuracy',6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict\npred22 = model2.predict_classes(validation_padded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Measuring the performance using various metrics: Precision, Recall, F1 score, Overall Accuracy\n\ncnf_matrix = confusion_matrix(valid_labels,pred22)\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');\n\n\nprecision_scoreM22=precision_score(valid_labels,pred22, average=None)\nprint(\"\\n Model-22 Precision_score\\n\", precision_scoreM22)\n\nrecall_scoreM22=recall_score(valid_labels,pred22, average=None)\nprint(\"\\n Model-22 Recall_score\\n\",recall_scoreM22)\n\naccuracy_scoreM22=accuracy_score(valid_labels,pred22)\nprint(\"\\n Model-22 Accuracy_score\\n\",accuracy_scoreM22)\n\nf1_scoreM22=f1_score(valid_labels,pred22, average=None)\nprint(\"\\n Model-22 F1_score\\n\",f1_scoreM22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict the test data\n\ntest_pred = model2.predict(test_padded) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'] = (test_pred > 0.5).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}